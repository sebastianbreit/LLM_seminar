{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final LLM class Project \n",
    "\n",
    "Authors: Luisa, Sebastian, Jan-Felix, Nion \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import os\n",
    "os.chdir(\"/Users/luisakurth/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################\n",
    "#LLM Set Up\n",
    "##################################################\n",
    "# this first part is from the course webpage\n",
    "# see: https://cogsciprag.github.io/LLM-implications/materials/session3\n",
    "# I don't know whether we are allowed to copy this verbatim\n",
    "# but it wouldn't be very much work to implement something similar anyways\n",
    "# of course, if possible, we should adapt it to more recent models\n",
    "\n",
    "# taken from: https://cogsciprag.github.io/LLM-implications/materials/session3\n",
    "\n",
    "# load the tokenizer & model for T5 & GPT2\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_T5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",\n",
    "                                                 pad_token_id=tokenizer_T5.eos_token_id)\n",
    "\n",
    "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_GPT2 = GPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
    "                                             pad_token_id=tokenizer_GPT2.eos_token_id)\n",
    "\n",
    "# convenience function for nicer output\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(s)\n",
    "\n",
    "\n",
    "def generate(prompt, model=\"T5\"):\n",
    "    if model == \"T5\":\n",
    "        model = model_T5\n",
    "        tokenizer = tokenizer_T5\n",
    "    else:\n",
    "        model = model_GPT2\n",
    "        tokenizer = tokenizer_GPT2\n",
    "    # encode context the generation is conditioned on\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    # get output with standard parameters\n",
    "    sample_output = model.generate(\n",
    "        input_ids,        # context to continue\n",
    "        do_sample=True,   # use sampling (not beam search (see below))\n",
    "        # return maximally 50 words (including the input given)\n",
    "        max_length=500,\n",
    "        top_k=0,          # just sample one word\n",
    "        top_p=1,          # consider all options\n",
    "        temperature=0.7   # soft-max temperature\n",
    "    )\n",
    "    return(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "#Experiment 1 Set-up \n",
    "###########################################\n",
    "\n",
    "def convert(text):\n",
    "    \"\"\"this function converts the experimental item into a prompt\n",
    "    :text param: experimental item/sentence\n",
    "    :output: prompt for model\n",
    "    \"\"\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = text+\"?\"\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def process_stimuli(convert_func):\n",
    "    \"\"\"\n",
    "    Processes a stimuli file and extracts \"thinks\" and \"announced\" prompts.\n",
    "    :param convert_func: A function to convert the prompts as needed.\n",
    "    :return: Lists of \"thinks\" and \"announced\" prompts.\n",
    "    \"\"\"\n",
    "    thinks = []\n",
    "    announced = []\n",
    "    all_lines = []\n",
    "    stimuli_exp1_file = open(\"stimuli_Exp1.txt\", 'r')\n",
    "    # open file (from: https://osf.io/9eg34/)\n",
    "\n",
    "    for l in stimuli_exp1_file.readlines():\n",
    "        all_lines.append(l)\n",
    "\n",
    "    for i in range(len(all_lines)//3):\n",
    "\n",
    "        # in the file, there is always one \"thinks\" line followed by one \"announced\" line and one blank line\n",
    "        line1 = all_lines.pop(0)\n",
    "        # remove first part since its an identifier\n",
    "        text1 = \" \".join(line1.split()[1:])\n",
    "        prompt1 = convert_func(text1)\n",
    "        thinks.append(prompt1)\n",
    "\n",
    "        line2 = all_lines.pop(0)\n",
    "        text2 = \" \".join(line2.split()[1:])\n",
    "        prompt2 = convert_func(text2)\n",
    "        announced.append(prompt2)\n",
    "\n",
    "        all_lines.pop(0)\n",
    "\n",
    "    return thinks, announced\n",
    "\n",
    "\n",
    "\n",
    "def extract_number(question, response):\n",
    "    \"\"\"This function defines whether the lower or higher or both or neither number of the question is present in the model response \n",
    "    \"\"\"\n",
    "    # this is still very rough and could be modified to reduce the risk of missing out some instances (e.g. by\n",
    "    # excluding more irrelevant punctuation marks, or including number word representations)\n",
    "    qs = question.replace(\"?\", \"\").split()\n",
    "    low = qs[-3]\n",
    "    high = qs[-1]\n",
    "    has_low = False\n",
    "    has_high = False\n",
    "    rs = response.replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "    for i in rs:\n",
    "        if i == low:\n",
    "            has_low = True\n",
    "        if i == high:\n",
    "            has_high = True\n",
    "\n",
    "    if has_low and has_high:\n",
    "        return \"both\"\n",
    "    elif has_low:\n",
    "        return \"low\"\n",
    "    elif has_high:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"neither\"\n",
    "\n",
    "\n",
    "def get_counts(N, thinks, announced, model=\"T5\"):\n",
    "    '''Counts the numbers in the model response. \n",
    "    :param N: number of participatants i.e. number of repeats for model \n",
    "    :param thinks: dataset with think sentences \n",
    "    :param announced: dataset with announced sentences\n",
    "    :param model: LLM model to use \n",
    "    :output: think_counts and announce_counts, in the format [counts of both numbers in answer, counts of low number, counts of high, counts of neither]\n",
    "    '''\n",
    "    think_counts = [0, 0, 0, 0]\n",
    "    announce_counts = [0, 0, 0, 0]\n",
    "\n",
    "    for p in range(N):  # Number of participants in the \"think\" condition\n",
    "        for tq in thinks:\n",
    "            response = generate(tq, model)\n",
    "            result = extract_number(tq, response)\n",
    "            if result == \"both\":\n",
    "                think_counts[0] += 1\n",
    "            elif result == \"low\":\n",
    "                think_counts[1] += 1\n",
    "            elif result == \"high\":\n",
    "                think_counts[2] += 1\n",
    "            else:\n",
    "                think_counts[3] += 1\n",
    "\n",
    "    for p in range(N):  # Number of participants in the \"announce\" condition\n",
    "        for aq in announced:\n",
    "            response = generate(aq, model)\n",
    "            result = extract_number(aq, response)\n",
    "            if result == \"both\":\n",
    "                announce_counts[0] += 1\n",
    "            elif result == \"low\":\n",
    "                announce_counts[1] += 1\n",
    "            elif result == \"high\":\n",
    "                announce_counts[2] += 1\n",
    "            else:\n",
    "                announce_counts[3] += 1\n",
    "\n",
    "    return think_counts, announce_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for all results: \n",
    "results = pd.DataFrame(columns=[\"Both\", \"Low\", \"High\", \"Neither\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition: [0, 34, 21, 5]\n",
      "The T5 model in the announce condition: [0, 37, 18, 5]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#baseline results\n",
    "##################\n",
    "\n",
    "thinks, announced = process_stimuli(convert)\n",
    "think_counts, announce_counts = get_counts(N=5, thinks=thinks, announced=announced)\n",
    "print(\"The T5 model in the think condition:\", think_counts)\n",
    "print(\"The T5 model in the announce condition:\", announce_counts)\n",
    "\n",
    "# results for one run: [0, 496, 393, 101], [0, 520, 410, 60] (will be different for other runs)\n",
    "# hence it happens at times that neither number is present in the response; it would probably be worth investigation\n",
    "# how this is caused (whether it's actually not there or whether it is not recognized)\n",
    "\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"baseline_T5_t\"] = think_counts\n",
    "results.loc[\"baseline_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with 1 shot learning: [0, 39, 19, 2]\n",
      "The T5 model in the announce condition with 1 shot learning [0, 36, 23, 1]\n",
      "The T5 model in the think condition with 2 shot learning: [0, 38, 20, 2]\n",
      "The T5 model in the announce condition with 2 shot learning: [0, 37, 23, 0]\n",
      "The T5 model in the think condition with 3 shot learning: [0, 37, 20, 3]\n",
      "The T5 model in the announce condition with 3 shot learning: [0, 36, 24, 0]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#1-shot learning\n",
    "#################\n",
    "\n",
    "def convert_oneshot(text):\n",
    "    \"\"\"Creates prompt with 1 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52. \"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_oneshot)\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=5, thinks=thinks, announced=announced)\n",
    "print(\"The T5 model in the think condition with 1 shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 1 shot learning\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"1shot_T5_t\"] = think_counts\n",
    "results.loc[\"1shot_T5_a\"] = announce_counts\n",
    "\n",
    "\n",
    "##################\n",
    "#2-shot learning\n",
    "#################\n",
    "def convert_twoshot(text):\n",
    "    \"\"\"Creates prompt with 2 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52.\"\n",
    "    example2 = \"Alex is a person from the US. Alex works at a cafe with Emma. Emma thinks that Alex drank ___ cups of coffee yesterday. Choose the more likely number: 3 or 5. The answer is 3. Alex is a person from the US. Alex works at a cafe with Emma. Emma announced to me that Alex drank ___ cups of coffee yesterday. Choose the more likely number:  3 or 5. The answer is 5.\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+example2+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_twoshot)\n",
    "think_counts, announce_counts = get_counts(N=5, thinks=thinks, announced=announced)\n",
    "print(\"The T5 model in the think condition with 2 shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 2 shot learning:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"2shot_T5_t\"] = think_counts\n",
    "results.loc[\"2shot_T5_a\"] = announce_counts\n",
    "\n",
    "\n",
    "##################\n",
    "#3-shot learning\n",
    "#################\n",
    "def convert_threeshot(text):\n",
    "    \"\"\"Creates prompt with 3 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52.\"\n",
    "    example2 = \"Alex is a person from the US. Alex works at a cafe with Emma. Emma thinks that Alex drank ___ cups of coffee yesterday. Choose the more likely number: 3 or 5. The answer is 3. Alex is a person from the US. Alex works at a cafe with Emma. Emma announced to me that Alex drank ___ cups of coffee yesterday. Choose the more likely number:  3 or 5. The answer is 5.\"\n",
    "    example3 = \"Michael is a man from the US. Michael lives in a hot climate. He thinks that the temperature reached ___ degrees Celsius yesterday.Choose the more likely number: 34 or 40. The answer is 34. Michael is a man from the US. Michael lives in a hot climate. He announced to me that the temperature reached ___ degrees Celsius yesterday. Choose the more likely number:  34 or 40. The answer is 40.\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+example2+example3+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_threeshot)\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=5, thinks=thinks, announced=announced)\n",
    "print(\"The T5 model in the think condition with 3 shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 3 shot learning:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"3shot_T5_t\"] = think_counts\n",
    "results.loc[\"3shot_T5_a\"] = announce_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with 1 extreme shot learning: [0, 42, 14, 4]\n",
      "The T5 model in the announce condition with 1 extreme shot learning [0, 35, 24, 1]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#1-shot-extreme learning\n",
    "#################\n",
    "\n",
    "def convert_oneshot_extreme(text):\n",
    "    \"\"\"Creates prompt with 1 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example_e = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 11 or 102. The answer is 11. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 11 or 102. The answer is 102. \"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example_e+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_oneshot_extreme)\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=5, thinks=thinks, announced=announced)\n",
    "print(\"The T5 model in the think condition with 1 extreme shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 1 extreme shot learning\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"1shot_extreme_T5_t\"] = think_counts\n",
    "results.loc[\"1shot_extreme_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with knowledge prompting : [0, 28, 31, 1]\n",
      "The T5 model in the announce condition with knowledge prompting: [0, 33, 26, 1]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#knowledge generation learning\n",
    "###############################\n",
    "\n",
    "#https://www.promptingguide.ai/techniques/knowledge\n",
    "\n",
    "def convert_know(text):\n",
    "    \"\"\"Creates prompt with a infomration before the question  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    knowledge = \"Knowledge: the lower number is just slighlty above the mean and the larger number is a lot above the mean.\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = knowledge+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_know)\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=5, thinks=thinks, announced=announced)\n",
    "print(\"The T5 model in the think condition with knowledge prompting :\", think_counts)\n",
    "print(\"The T5 model in the announce condition with knowledge prompting:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"know_T5_t\"] = think_counts\n",
    "results.loc[\"know_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Both  Low  High  Neither\n",
      "baseline_T5_t          0   34    21        5\n",
      "baseline_T5_a          0   37    18        5\n",
      "1shot_T5_t             0   39    19        2\n",
      "1shot_T5_a             0   36    23        1\n",
      "2shot_T5_t             0   38    20        2\n",
      "2shot_T5_a             0   37    23        0\n",
      "3shot_T5_t             0   37    20        3\n",
      "3shot_T5_a             0   36    24        0\n",
      "1shot_extreme_T5_t     0   42    14        4\n",
      "1shot_extreme_T5_a     0   35    24        1\n",
      "know_T5_t              0   28    31        1\n",
      "know_T5_a              0   33    26        1\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
