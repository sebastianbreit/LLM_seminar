{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final LLM class Project \n",
    "\n",
    "Authors: Luisa, Sebastian, Jan-Felix, Nion \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisakurth/opt/anaconda3/envs/LLM/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.chdir(\"/Users/luisakurth/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################\n",
    "#LLM Set Up\n",
    "##################################################\n",
    "# this first part is from the course webpage\n",
    "# see: https://cogsciprag.github.io/LLM-implications/materials/session3\n",
    "# I don't know whether we are allowed to copy this verbatim\n",
    "# but it wouldn't be very much work to implement something similar anyways\n",
    "# of course, if possible, we should adapt it to more recent models\n",
    "\n",
    "# taken from: https://cogsciprag.github.io/LLM-implications/materials/session3\n",
    "\n",
    "# load the tokenizer & model for T5 & GPT2\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_T5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",\n",
    "                                                 pad_token_id=tokenizer_T5.eos_token_id)\n",
    "\n",
    "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_GPT2 = GPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
    "                                             pad_token_id=tokenizer_GPT2.eos_token_id)\n",
    "\n",
    "# convenience function for nicer output\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(s)\n",
    "\n",
    "\n",
    "def generate(prompt, model=\"T5\"):\n",
    "    if model == \"T5\":\n",
    "        model = model_T5\n",
    "        tokenizer = tokenizer_T5\n",
    "    else:\n",
    "        model = model_GPT2\n",
    "        tokenizer = tokenizer_GPT2\n",
    "    # encode context the generation is conditioned on\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    # get output with standard parameters\n",
    "    sample_output = model.generate(\n",
    "        input_ids,        # context to continue\n",
    "        do_sample=True,   # use sampling (not beam search (see below))\n",
    "        # return maximally 50 words (including the input given)\n",
    "        max_length=500,\n",
    "        top_k=0,          # just sample one word\n",
    "        top_p=1,          # consider all options\n",
    "        temperature=0.7   # soft-max temperature\n",
    "    )\n",
    "    return(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "#Experiment Set-up \n",
    "###########################################\n",
    "\n",
    "def convert_exp1(text):\n",
    "    \"\"\"this function converts the experimental item into a prompt\n",
    "    :text param: experimental item/sentence\n",
    "    :output: prompt for model\n",
    "    \"\"\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = text+\"?\"\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def process_stimuli(convert_func, exp_stimuli):\n",
    "    \"\"\"\n",
    "    Processes a stimuli file and extracts \"thinks\" and \"announced\" prompts in two sepetate datasets.\n",
    "    :param convert_func: requires function to convert the prompts as needed.\n",
    "    :param exp_stimuli: requires a string of experimental txt file \n",
    "    :return: List of \"thinks\" and list of \"announced\" prompts.\n",
    "    \"\"\"\n",
    "    thinks = []\n",
    "    announced = []\n",
    "    all_lines = []\n",
    "    stimuli_exp1_file = open(exp_stimuli, 'r')\n",
    "    # open file (from: https://osf.io/9eg34/)\n",
    "\n",
    "    for l in stimuli_exp1_file.readlines():\n",
    "        all_lines.append(l)\n",
    "\n",
    "    for i in range(len(all_lines)//3):\n",
    "\n",
    "        # in the file, there is always one \"thinks\" line followed by one \"announced\" line and one blank line\n",
    "        line1 = all_lines.pop(0)\n",
    "        # remove first part since its an identifier\n",
    "        text1 = \" \".join(line1.split()[1:])\n",
    "        prompt1 = convert_func(text1)\n",
    "        thinks.append(prompt1)\n",
    "\n",
    "        line2 = all_lines.pop(0)\n",
    "        text2 = \" \".join(line2.split()[1:])\n",
    "        prompt2 = convert_func(text2)\n",
    "        announced.append(prompt2)\n",
    "\n",
    "        all_lines.pop(0)\n",
    "\n",
    "    return thinks, announced\n",
    "\n",
    "\n",
    "\n",
    "def process_stimuli_exp4(convert_func, exp_stimuli):\n",
    "    \"\"\"\n",
    "    Processes a stimuli file for experiment 4.\n",
    "    :param convert_func: requires function to convert the prompts as needed.\n",
    "    :param exp_stimuli: requires a string of experimental txt file \n",
    "    :return: List of \"high\", list of \"mid\", and list of \"low\" prompts.\n",
    "    \"\"\"\n",
    "    high = []\n",
    "    mid = []\n",
    "    low = []\n",
    "    all_lines = []\n",
    "    stimuli_exp1_file = open(exp_stimuli, 'r')\n",
    "    # open file (from: https://osf.io/9eg34/)\n",
    "\n",
    "    for l in stimuli_exp1_file.readlines():\n",
    "        all_lines.append(l)\n",
    "\n",
    "    for i in range(len(all_lines)//4):\n",
    "\n",
    "        # in the file, there is always one \"high\" line followed by one \"mid\" line, on \"low\" line, and one blank line\n",
    "        line1 = all_lines.pop(0)\n",
    "        # remove first part since its an identifier\n",
    "        text1 = \" \".join(line1.split()[1:])\n",
    "        prompt1 = convert_func(text1)\n",
    "        high.append(prompt1)\n",
    "\n",
    "        line2 = all_lines.pop(0)\n",
    "        text2 = \" \".join(line2.split()[1:])\n",
    "        prompt2 = convert_func(text2)\n",
    "        mid.append(prompt2)\n",
    "\n",
    "        line3 = all_lines.pop(0)\n",
    "        text3 = \" \".join(line3.split()[1:])\n",
    "        prompt3 = convert_func(text3)\n",
    "        low.append(prompt3)\n",
    "\n",
    "        all_lines.pop(0)\n",
    "\n",
    "    return high, mid, low\n",
    "\n",
    "\n",
    "\n",
    "def extract_number(question, response):\n",
    "    \"\"\"This function defines whether the lower or higher or both or neither number of the question is present in the model response \n",
    "    \"\"\"\n",
    "    # this is still very rough and could be modified to reduce the risk of missing out some instances (e.g. by\n",
    "    # excluding more irrelevant punctuation marks, or including number word representations)\n",
    "    qs = question.replace(\"?\", \"\").split()\n",
    "    low = qs[-3]\n",
    "    high = qs[-1]\n",
    "    has_low = False\n",
    "    has_high = False\n",
    "    rs = response.replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "    for i in rs:\n",
    "        if i == low:\n",
    "            has_low = True\n",
    "        if i == high:\n",
    "            has_high = True\n",
    "\n",
    "    if has_low and has_high:\n",
    "        return \"both\"\n",
    "    elif has_low:\n",
    "        return \"low\"\n",
    "    elif has_high:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"neither\"\n",
    "\n",
    "\n",
    "\n",
    "def get_counts(N, condition1prompts, condition2prompts, model=\"T5\"):\n",
    "    '''Counts the numbers in the model response. \n",
    "    :param N: number of participatants i.e. number of repeats for model \n",
    "    :param condition1prompts: dataset with e.g. think sentences \n",
    "    :param condition2prompts: dataset with e.g. announced sentences\n",
    "    :param model: LLM model to use \n",
    "    :output: condition1_counts and condition2_counts, in the format [counts of both numbers in answer, counts of low number, counts of high, counts of neither]\n",
    "    '''\n",
    "    condition1_counts = [0, 0, 0, 0]\n",
    "    condition2_counts = [0, 0, 0, 0]\n",
    "\n",
    "    for p in range(N):  # Number of participants in the \"think\" condition\n",
    "        for tq in condition1prompts:\n",
    "            response = generate(tq, model)\n",
    "            result = extract_number(tq, response)\n",
    "            if result == \"both\":\n",
    "                condition1_counts[0] += 1\n",
    "            elif result == \"low\":\n",
    "                condition1_counts[1] += 1\n",
    "            elif result == \"high\":\n",
    "                condition1_counts[2] += 1\n",
    "            else:\n",
    "                condition1_counts[3] += 1\n",
    "\n",
    "    for p in range(N):  # Number of participants in the \"announce\" condition\n",
    "        for aq in condition2prompts:\n",
    "            response = generate(aq, model)\n",
    "            result = extract_number(aq, response)\n",
    "            if result == \"both\":\n",
    "                condition2_counts[0] += 1\n",
    "            elif result == \"low\":\n",
    "                condition2_counts[1] += 1\n",
    "            elif result == \"high\":\n",
    "                condition2_counts[2] += 1\n",
    "            else:\n",
    "                condition2_counts[3] += 1\n",
    "\n",
    "    return condition1_counts, condition2_counts\n",
    "\n",
    "\n",
    "def get_response(N, condition1prompts, condition2prompts, name_cond1, name_cond2, model=\"T5\"):\n",
    "    '''Reports the prompt, response and response classifiction (into low, high, both, neither) of the model for N trials\n",
    "    :param N: number of participatants i.e. number of repeats for model \n",
    "    :param condition1prompts: dataset with condition 1 sentences as list\n",
    "    :param condition2prompts: dataset with condition 2 sentences as list \n",
    "    :param name_cond1: name of condtion that shows up in the output table \n",
    "    :param condition2prompts: name of condtion that shows up in the output table \n",
    "    :param model: LLM model to use \n",
    "    :output: dataframe condition1output and dataframe condition2output\n",
    "    '''\n",
    "    condition1output = pd.DataFrame(columns=[\"Condition\", \"Prompt\", \"Response\", \"Number\"])\n",
    "    condition2output = pd.DataFrame(columns=[\"Condition\", \"Prompt\", \"Response\", \"Number\"])\n",
    "\n",
    "\n",
    "    for p in range(N): \n",
    "        \n",
    "        for tq in condition1prompts:\n",
    "            response = generate(tq, model)\n",
    "            result = extract_number(tq, response)\n",
    "            row = {\"Condition\": name_cond1, \"Prompt\": tq, \"Response\": response, \"Number\": result}\n",
    "            condition1output = condition1output.append(row, ignore_index=True)\n",
    "\n",
    "        for tq in condition2prompts:\n",
    "            response = generate(tq, model)\n",
    "            result = extract_number(tq, response)\n",
    "            row = {\"Condition\": name_cond2, \"Prompt\": tq, \"Response\": response, \"Number\": result}\n",
    "            condition2output = condition2output.append(row, ignore_index=True)\n",
    "\n",
    "    return condition1output, condition2output\n",
    "\n",
    "def count_string_occurrences(dataframe, column_name):\n",
    "    # Get the counts of each string in the specified column\n",
    "    string_counts = dataframe[column_name].value_counts()\n",
    "\n",
    "    # Create a dictionary to store the string counts\n",
    "    counts = {}\n",
    "\n",
    "    # Iterate over the unique strings and their counts\n",
    "    for string, count in string_counts.items():\n",
    "        counts[string] = count\n",
    "\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results of different prompting strategies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for all counts results: \n",
    "results = pd.DataFrame(columns=[\"Both\", \"Low\", \"High\", \"Neither\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#baseline results\n",
    "##################\n",
    "\n",
    "#get the prompts for the 2 condtions in 2 lists\n",
    "thinks, announced = process_stimuli(convert_exp1,\"stimuli_Exp1.txt\")\n",
    "\n",
    "#get the full responses to look at \n",
    "think_response, announce_response = get_response(N=1,condition1prompts=thinks, condition2prompts= announced, name_cond1=\"baseline_T5_t\", name_cond2=\"baseline_T5_a\")\n",
    "think_resp_counts = count_string_occurrences(think_response, \"Number\")\n",
    "announce_resp_counts = count_string_occurrences(announce_response, \"Number\")\n",
    "\n",
    "print(\"The T5 model in the think condition:\",think_resp_counts )\n",
    "print(\"The T5 model in the announce condition:\", announce_resp_counts)\n",
    "\n",
    "\n",
    "#get just counts for N = 10\n",
    "think_counts, announce_counts = get_counts(N=1,condition1prompts=thinks, condition2prompts= announced)\n",
    "\n",
    "print(\"The T5 model in the think condition:\", think_counts)\n",
    "print(\"The T5 model in the announce condition:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Counts_Results DataFrame\n",
    "results.loc[\"baseline_T5_t\"] = think_counts\n",
    "results.loc[\"baseline_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with 1 shot learning: [0, 8, 4, 0]\n",
      "The T5 model in the announce condition with 1 shot learning [0, 7, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#1-shot learning\n",
    "#################\n",
    "\n",
    "def convert_oneshot(text):\n",
    "    \"\"\"Creates prompt with 1 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52. \"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_oneshot,\"stimuli_Exp1.txt\")\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=1, condition1prompts=thinks,  condition2prompts=announced)\n",
    "print(\"The T5 model in the think condition with 1 shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 1 shot learning\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"1shot_T5_t\"] = think_counts\n",
    "results.loc[\"1shot_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with 1 shot learning: [0, 39, 19, 2]\n",
      "The T5 model in the announce condition with 1 shot learning [0, 36, 23, 1]\n",
      "The T5 model in the think condition with 2 shot learning: [0, 38, 20, 2]\n",
      "The T5 model in the announce condition with 2 shot learning: [0, 37, 23, 0]\n",
      "The T5 model in the think condition with 3 shot learning: [0, 37, 20, 3]\n",
      "The T5 model in the announce condition with 3 shot learning: [0, 36, 24, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##################\n",
    "#2-shot learning\n",
    "#################\n",
    "def convert_twoshot(text):\n",
    "    \"\"\"Creates prompt with 2 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52.\"\n",
    "    example2 = \"Alex is a person from the US. Alex works at a cafe with Emma. Emma thinks that Alex drank ___ cups of coffee yesterday. Choose the more likely number: 3 or 5. The answer is 3. Alex is a person from the US. Alex works at a cafe with Emma. Emma announced to me that Alex drank ___ cups of coffee yesterday. Choose the more likely number:  3 or 5. The answer is 5.\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+example2+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_twoshot, \"stimuli_Exp1.txt\")\n",
    "think_counts, announce_counts = get_counts(N=5, condition1prompts=thinks, condition2prompts=announced)\n",
    "print(\"The T5 model in the think condition with 2 shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 2 shot learning:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"2shot_T5_t\"] = think_counts\n",
    "results.loc[\"2shot_T5_a\"] = announce_counts\n",
    "\n",
    "\n",
    "##################\n",
    "#3-shot learning\n",
    "#################\n",
    "def convert_threeshot(text):\n",
    "    \"\"\"Creates prompt with 3 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52.\"\n",
    "    example2 = \"Alex is a person from the US. Alex works at a cafe with Emma. Emma thinks that Alex drank ___ cups of coffee yesterday. Choose the more likely number: 3 or 5. The answer is 3. Alex is a person from the US. Alex works at a cafe with Emma. Emma announced to me that Alex drank ___ cups of coffee yesterday. Choose the more likely number:  3 or 5. The answer is 5.\"\n",
    "    example3 = \"Michael is a man from the US. Michael lives in a hot climate. He thinks that the temperature reached ___ degrees Celsius yesterday.Choose the more likely number: 34 or 40. The answer is 34. Michael is a man from the US. Michael lives in a hot climate. He announced to me that the temperature reached ___ degrees Celsius yesterday. Choose the more likely number:  34 or 40. The answer is 40.\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+example2+example3+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_threeshot, \"stimuli_Exp1.txt\")\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=5, condition1prompts=thinks, condition2prompts=announced)\n",
    "print(\"The T5 model in the think condition with 3 shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 3 shot learning:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"3shot_T5_t\"] = think_counts\n",
    "results.loc[\"3shot_T5_a\"] = announce_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with 1 extreme shot learning: [0, 8, 3, 1]\n",
      "The T5 model in the announce condition with 1 extreme shot learning [0, 8, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#1-shot-extreme learning\n",
    "#################\n",
    "\n",
    "def convert_oneshot_extreme(text):\n",
    "    \"\"\"Creates prompt with 1 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example_e = \"Rachel is a librarian and works with Mark. Mark thinks that Rachel read ___ books last month. Choose the more likely number: 11 or 102. The answer is 11. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark announced to me that Rachel read ___ books last month. Choose the more likely number: 11 or 102. The answer is 102. \"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example_e+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_oneshot_extreme, \"stimuli_Exp1.txt\")\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=1, condition1prompts=thinks, condition2prompts=announced)\n",
    "print(\"The T5 model in the think condition with 1 extreme shot learning:\", think_counts)\n",
    "print(\"The T5 model in the announce condition with 1 extreme shot learning\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"1shot_extreme_T5_t\"] = think_counts\n",
    "results.loc[\"1shot_extreme_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the think condition with knowledge prompting : [0, 28, 31, 1]\n",
      "The T5 model in the announce condition with knowledge prompting: [0, 33, 26, 1]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#knowledge generation learning\n",
    "###############################\n",
    "\n",
    "#https://www.promptingguide.ai/techniques/knowledge\n",
    "\n",
    "def convert_know(text):\n",
    "    \"\"\"Creates prompt with a infomration before the question  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    knowledge = \"Knowledge: the lower number is just slighlty above the mean and the larger number is a lot above the mean.\"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = knowledge+text+\"?\"\n",
    "    return text\n",
    "\n",
    "thinks, announced = process_stimuli(convert_know, \"stimuli_Exp1.txt\")\n",
    "\n",
    "think_counts, announce_counts = get_counts(N=5, condition1prompts=thinks, condition2prompts=announced)\n",
    "print(\"The T5 model in the think condition with knowledge prompting :\", think_counts)\n",
    "print(\"The T5 model in the announce condition with knowledge prompting:\", announce_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results.loc[\"know_T5_t\"] = think_counts\n",
    "results.loc[\"know_T5_a\"] = announce_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Both  Low  High  Neither\n",
      "baseline_T5_t          0   34    21        5\n",
      "baseline_T5_a          0   37    18        5\n",
      "1shot_T5_t             0   39    19        2\n",
      "1shot_T5_a             0   36    23        1\n",
      "2shot_T5_t             0   38    20        2\n",
      "2shot_T5_a             0   37    23        0\n",
      "3shot_T5_t             0   37    20        3\n",
      "3shot_T5_a             0   36    24        0\n",
      "1shot_extreme_T5_t     0   42    14        4\n",
      "1shot_extreme_T5_a     0   35    24        1\n",
      "know_T5_t              0   28    31        1\n",
      "know_T5_a              0   33    26        1\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for all results of Exp2: \n",
    "results_exp2 = pd.DataFrame(columns=[\"Both\", \"Low\", \"High\", \"Neither\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the out of blue condition: [0, 35, 24, 9]\n",
      "The T5 model in the when asked condition: [0, 45, 17, 6]\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "#Baseline \n",
    "###########################################\n",
    "\n",
    "def convert_exp2(text):\n",
    "    \"\"\"This function converts the experimental item into a prompt.\n",
    "    :text param: experimental item/sentence\n",
    "    :output: prompt for model\n",
    "    \"\"\"\n",
    "    text = text.replace(\"XXX\", \". Choose the more likely number:\", 1)\n",
    "    text = text.replace(\"XXX\", \"or\", 1)\n",
    "    text = text + \"?\"\n",
    "    return text\n",
    "\n",
    "\n",
    "blue, asked = process_stimuli(convert_exp2, \"stimuli_Exp2.txt\")\n",
    "blue_counts, asked_counts = get_counts(N=2, condition1prompts=blue, condition2prompts=asked)\n",
    "\n",
    "print(\"The T5 model in the out of blue condition:\", blue_counts)\n",
    "print(\"The T5 model in the when asked condition:\", asked_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results_exp2.loc[\"exp2_T5_blue\"] = blue_counts\n",
    "results_exp2.loc[\"exp2_T5_asked\"] = asked_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the out of blue condition: [0, 0, 21, 47]\n",
      "The T5 model in the when asked condition: [0, 0, 20, 48]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#1-shot learning\n",
    "#################\n",
    "\n",
    "def convert_exp2_oneshot(text):\n",
    "    \"\"\"Creates prompt with 1 example before the actual task  \n",
    "    input: experimental item \n",
    "    output: Exmaple + Experimental item Task (Choose the more likely number)\n",
    "    \"\"\"\n",
    "    example1 = \"Rachel is a librarian and works with Mark. Mark said out of the blue that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 52. Rachel is a woman from the US. Rachel is a librarian and works with Mark. Mark said, when asked about it, that Rachel read ___ books last month. Choose the more likely number: 45 or 52. The answer is 45. \"\n",
    "    text = text.replace(\n",
    "        \"XXX\", \". Choose the more likely number:\").replace(\"YYYY\", \"or\")\n",
    "    text = example1+text\n",
    "    return text\n",
    "\n",
    "blue, asked = process_stimuli(convert_exp2_oneshot, \"stimuli_Exp2.txt\")\n",
    "blue_counts, asked_counts = get_counts(N=2, condition1prompts=blue, condition2prompts=asked)\n",
    "\n",
    "print(\"The T5 model in the out of blue condition:\", blue_counts)\n",
    "print(\"The T5 model in the when asked condition:\", asked_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results_exp2.loc[\"exp2_T5_blue_oneshot\"] = blue_counts\n",
    "results_exp2.loc[\"exp2_T5_aske_onshot\"] = asked_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Both  Low  High  Neither\n",
      "exp2_T5_blue      0   38    26        4\n",
      "exp2_T5_asked     0   38    27        3\n"
     ]
    }
   ],
   "source": [
    "print(results_exp2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 Model\n",
    "\n",
    "note: there is a double enter in the stimuli data (after the 24th item set) which needs to be removed, otherwise the follwing will nut work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for all results of Exp2: \n",
    "results_exp3 = pd.DataFrame(columns=[\"Both\", \"Low\", \"High\", \"Neither\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model in the everyone condition: [0, 22, 9, 3]\n",
      "The T5 model in the me condition: [0, 21, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "#Baseline \n",
    "###########################################\n",
    "\n",
    "def convert_exp3(text):\n",
    "    \"\"\"This function converts the experimental item into a prompt.\n",
    "    :text param: experimental item/sentence\n",
    "    :output: prompt for model\n",
    "    \"\"\"\n",
    "    text = text.replace(\"XXX\", \". Choose the more likely number:\", 1)\n",
    "    text = text.replace(\"XXX\", \"or\", 1)\n",
    "    text = text #+ \"?\"\n",
    "    return text\n",
    "\n",
    "\n",
    "everyone, me = process_stimuli(convert_exp3, \"stimuli_Exp3.txt\")\n",
    "everyone_counts, me_counts = get_counts(N=1, condition1prompts=everyone[:], condition2prompts=me[:])\n",
    "\n",
    "print(\"The T5 model in the everyone condition:\", everyone_counts)\n",
    "print(\"The T5 model in the me condition:\", me_counts)\n",
    "\n",
    "# Append the new row to the Results DataFrame\n",
    "results_exp2.loc[\"exp3_T5_everyone\"] = everyone_counts\n",
    "results_exp2.loc[\"exp3_T5_me\"] = me_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
